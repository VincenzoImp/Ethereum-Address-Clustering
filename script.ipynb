{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install web3 -q\n",
    "%pip install pandas -q\n",
    "%pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from web3.middleware import geth_poa_middleware\n",
    "from multiprocessing import Pool, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(datafile):\n",
    "    df = pd.read_csv(datafile)\n",
    "    block_heigth = df[\"block_number_remove\"].max()\n",
    "    address_df = df.\\\n",
    "        sort_values([\"block_number_remove\"]).\\\n",
    "        drop_duplicates(subset=[\"from\"], keep=\"last\").\\\n",
    "        reset_index(drop=True)\\\n",
    "        [[\"from\", \"block_number_remove\"]].\\\n",
    "        rename({\"from\":\"address\", \"block_number_remove\":\"use_untill\"}, axis=\"columns\")\n",
    "    return address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi(address_df, edgefile, nodefile, w3, depth=2, mode=\"w\", store=\"received\", use_untill=True): \n",
    "    \"\"\"\n",
    "    depth: (int >=0) ultimo livello da archiviare compreso\n",
    "    mode: (\"w\", \"a\") \n",
    "        \"w\": dal livello 0 a depth compreso sovrascrivendo i file\n",
    "        \"a\": dall'ultimo livello gia' archiviato nei file fino a depth compreso appendendo nei file\n",
    "    store: (\"received\", \"sent\", \"both\")\n",
    "        \"received\": per la creazione di un nuovo livello vengono archiviate solo le transazioni ricevute dagli address del livello corrente\n",
    "        \"sent\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate dagli address del livello corrente\n",
    "        \"both\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate e ricevute dagli address del livello corrente\n",
    "    use_untill: (True, False)\n",
    "        True: le transazioni, una volta filtrate da store, vengono filtrate da block_number <= use_untill\n",
    "        False: le transazioni, una volta filtrate da store, vengono accettate tutte\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_block_heigth e step sono due parametri da poter tarare con dei MA:\n",
    "        siccome nel preprocessing vengono accettati tutti gli address del cvs, max_block_heigth non puo' essere minore di address_df[\"use_untill\"].max(), per evitare di disegnare nel grafo alcuni nodi incorretti\n",
    "        max_block_heigth puo' essere cio' che ci pare al netto del vincolo appena citato solo se \n",
    "    di default max_block_heigth e step sono:\n",
    "        max_block_heigth = address_df[\"use_untill\"].max()\n",
    "        step = 1000\n",
    "    \"\"\"\n",
    "    def task(start, lock): \n",
    "        new_level_address_subset = set()\n",
    "        new_level_address_subdf = pd.DataFrame.from_dict({\"address\": [], \"use_untill\": [], \"level\": []})\n",
    "        for block_number in range(min(start+step-1, max_block_heigth), start-1, -1):\n",
    "            block = w3.eth.get_block(block_number)\n",
    "            for transaction in block.transactions[::-1]:\n",
    "                tx = {**w3.eth.get_transaction(transaction.hex()), **w3.eth.get_transaction_receipt(transaction.hex())}\n",
    "                if tx[\"status\"] == 1:\n",
    "                    address_to_add = None\n",
    "                    if store == \"received\" and use_untill == True:\n",
    "                        if tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    elif store == \"received\" and use_untill == False:\n",
    "                        if tx[\"to\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    elif store == \"sent\" and use_untill == True:\n",
    "                        if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                    elif store == \"sent\" and use_untill == False:\n",
    "                        if tx[\"from\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                    elif store == \"both\" and use_untill == True:\n",
    "                        if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                        elif tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    elif store == \"both\" and use_untill == False:\n",
    "                        if tx[\"from\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                        elif tx[\"to\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    if address_to_add != None:\n",
    "                        with lock:\n",
    "                            tx_file.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                tx[\"from\"], tx[\"to\"], tx[\"value\"], tx[\"effectiveGasPrice\"], tx[\"gasUsed\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"], new_level\n",
    "                                ))\n",
    "                        if address_to_add not in new_level_address_subset:\n",
    "                            new_level_address_subset.add(address_to_add)\n",
    "                            row = pd.DataFrame.from_dict({\"address\": [address_to_add], \"use_untill\": [block_number], \"level\": [new_level]})\n",
    "                            new_level_address_subdf = pd.concat([new_level_address_subdf, row], ignore_index=True)\n",
    "        return new_level_address_subdf\n",
    "    if mode == \"w\":\n",
    "        if \"level\" not in address_df.columns:\n",
    "            address_df[\"level\"] = 0\n",
    "        address_df.to_csv(nodefile, index=False)\n",
    "        with open(edgefile, \"w\", encoding=\"UTF8\") as tx_file:\n",
    "            tx_file.write(\"from,to,value,effectiveGasPrice,gasUsed,hash,input,blockNumber,level\\n\")\n",
    "            curr_level = 0\n",
    "            curr_level_address_set = set(address_df[\"address\"].values)\n",
    "            curr_level_address_df = address_df\n",
    "            while curr_level < depth:\n",
    "                max_block_heigth = address_df[\"use_untill\"].max()\n",
    "                step = 1000\n",
    "                # step = max_block_heigth//15000\n",
    "                new_level = curr_level + 1\n",
    "                #generate new level of edges and nodes\n",
    "                with Manager() as manager:\n",
    "                    lock = manager.Lock()\n",
    "                    with Pool() as pool:\n",
    "                        items = [(i, lock) for i in range(0, max_block_heigth+1, step)]\n",
    "                        new_level_address_subdf_list = pool.starmap_async(task, items)\n",
    "                        new_level_address_subdf_list.wait()\n",
    "                new_level_address_df = pd.concat(new_level_address_subdf_list.get())\n",
    "                #elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\n",
    "                new_level_address_df = new_level_address_df.\\\n",
    "                    sort_values([\"use_untill\"]).\\\n",
    "                    drop_duplicates(subset=[\"address\"], keep=\"last\").\\\n",
    "                    reset_index(drop=True)\\\n",
    "                #elimina da new_level_address_df le righe che hanno address presenti in address_df\n",
    "                new_level_address_df = new_level_address_df[~new_level_address_df[\"address\"].isin(address_df[\"address\"])]\n",
    "                address_df = pd.concat([new_level_address_df, address_df])\n",
    "                new_level_address_df.to_csv(nodefile, mode=\"a\", header=False, index=False)\n",
    "                curr_level += 1\n",
    "                curr_level_address_set = set(new_level_address_df[\"address\"].values)\n",
    "                curr_level_address_df = new_level_address_df\n",
    "    return curr_level_address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_url = 'https://mainnet.infura.io/v3/e0a4e987f3ff4f4fa9aa21bb08f09ef5'\n",
    "bsc_url = \"https://bsc-dataseed.binance.org/\"\n",
    "\n",
    "eth_datafile = 'one_day_exit_scam_eth.csv'\n",
    "bsc_datafile = 'one_day_exit_scam_bsc.csv'\n",
    "\n",
    "eth_edgefile = 'tx_eth.csv'\n",
    "bsc_edgefile = 'tx_bsc.csv'\n",
    "\n",
    "eth_nodefile = 'address_eth.csv'\n",
    "bsc_nodefile = 'address_bsc.csv'\n",
    "\n",
    "eth_w3 = Web3(Web3.HTTPProvider(eth_url))\n",
    "bsc_w3 = Web3(Web3.HTTPProvider(bsc_url))\n",
    "bsc_w3.middleware_onion.inject(geth_poa_middleware, layer=0)\n",
    "\n",
    "eth_address_df = preprocessing(eth_datafile)\n",
    "bsc_address_df = preprocessing(bsc_datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'multi.<locals>.task'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m multi(eth_address_df, eth_edgefile, eth_nodefile, eth_w3)\n",
      "\u001b[1;32m/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb Cell 6\u001b[0m in \u001b[0;36mmulti\u001b[0;34m(address_df, edgefile, nodefile, w3, depth, mode, store, use_untill)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         new_level_address_subdf_list \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mstarmap_async(task, items)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         new_level_address_subdf_list\u001b[39m.\u001b[39mwait()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m new_level_address_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(new_level_address_subdf_list\u001b[39m.\u001b[39;49mget())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m#elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m new_level_address_df \u001b[39m=\u001b[39m new_level_address_df\u001b[39m.\u001b[39m\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     sort_values([\u001b[39m\"\u001b[39m\u001b[39muse_untill\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39m\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     drop_duplicates(subset\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maddress\u001b[39m\u001b[39m\"\u001b[39m], keep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39m\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincenzo/Documents/GitHub/Ethereum-Address-Clustering/script.ipynb#W5sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     reset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\\\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/multiprocessing/pool.py:537\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     put(task)\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     job, idx \u001b[39m=\u001b[39m task[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/multiprocessing/connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/anaconda3/envs/stable/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'multi.<locals>.task'"
     ]
    }
   ],
   "source": [
    "multi(eth_address_df, eth_edgefile, eth_nodefile, eth_w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi(bsc_address_df, eth_edgefile, eth_nodefile, eth_w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'effectiveGasPrice': 22440056776, 'gasUsed': 21000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0xdE1c59Bc25D806aD9DdCbe246c4B5e5505645718,0x9008D19f58AAbD9eD0D60971565AA8510560ab41,0,0x3dac2080b4c423029fcc9c916bc430cde441badfe736fc6d1fe9325348af80fd,0x13d79a0b,14000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from web3 import Web3\n",
    "eth_url = 'https://mainnet.infura.io/v3/e0a4e987f3ff4f4fa9aa21bb08f09ef5'\n",
    "w3 = Web3(Web3.HTTPProvider(eth_url))\n",
    "\n",
    "transaction = w3.eth.get_block(14000000).transactions[0]\n",
    "\n",
    "tx = w3.eth.get_transaction(transaction.hex())\n",
    "\n",
    "print(\"{},{},{},{},{},{}\\n\".format(\n",
    "    tx[\"from\"], tx[\"to\"], tx[\"value\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Courses</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PySpark</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hadoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55days</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Courses      Fee Duration  Discount\n",
       "1  PySpark  25000.0     None    2300.0\n",
       "2   Hadoop      NaN   55days    1000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\"],\n",
    "    'Fee' :[22000,25000,np.nan,24000],\n",
    "    'Duration':['30day',None,'55days',np.nan],\n",
    "    'Discount':[1000,2300,1000,np.nan]\n",
    "}\n",
    "df = pd.DataFrame(technologies)\n",
    "\n",
    "t = {'Courses':[\"Spark\",\"Python\"]}\n",
    "df[~df['Courses'].isin(t['Courses'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/955 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m datafile, edgefile, nodefile, w3 \u001b[39m=\u001b[39m eth_datafile, eth_edgefile, eth_nodefile, eth_w3\n\u001b[1;32m    142\u001b[0m \u001b[39m#datafile, edgefile, nodefile, w3 = bsc_datafile, bsc_edgefile, bsc_nodefile, bsc_w3\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m multi(preprocessing(datafile), edgefile, nodefile)\n",
      "Cell \u001b[0;32mIn [1], line 50\u001b[0m, in \u001b[0;36mmulti\u001b[0;34m(address_df, edgefile, nodefile, depth, mode, store, use_untill)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mwith\u001b[39;00m Pool() \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m     49\u001b[0m         items \u001b[39m=\u001b[39m [(i, lock, store, use_untill, step, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df, pbar) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, max_block_heigth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, step)]\n\u001b[0;32m---> 50\u001b[0m         new_level_address_subdf_list \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(task, items)\n\u001b[1;32m     51\u001b[0m new_level_address_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(new_level_address_subdf_list)\n\u001b[1;32m     52\u001b[0m \u001b[39m#elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     put(task)\n\u001b[1;32m    541\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[39m=\u001b[39m task[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "#from web3.middleware import geth_poa_middleware\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def multi(address_df, edgefile, nodefile, depth=10, mode=\"w\", store=\"received\", use_untill=True): \n",
    "    \"\"\"\n",
    "    depth: (int >=0) ultimo livello da archiviare compreso\n",
    "    mode: (\"w\", \"a\") \n",
    "        \"w\": dal livello 0 a depth compreso sovrascrivendo i file\n",
    "        \"a\": dall'ultimo livello gia' archiviato nei file fino a depth compreso appendendo nei file\n",
    "    store: (\"received\", \"sent\", \"both\")\n",
    "        \"received\": per la creazione di un nuovo livello vengono archiviate solo le transazioni ricevute dagli address del livello corrente\n",
    "        \"sent\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate dagli address del livello corrente\n",
    "        \"both\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate e ricevute dagli address del livello corrente\n",
    "    use_untill: (True, False)\n",
    "        True: le transazioni, una volta filtrate da store, vengono filtrate da block_number <= use_untill\n",
    "        False: le transazioni, una volta filtrate da store, vengono accettate tutte\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_block_heigth e step sono due parametri da poter tarare con dei MA:\n",
    "        siccome nel preprocessing vengono accettati tutti gli address del cvs, max_block_heigth non puo' essere minore di address_df[\"use_untill\"].max(), per evitare di disegnare nel grafo alcuni nodi incorretti\n",
    "        max_block_heigth puo' essere cio' che ci pare al netto del vincolo appena citato solo se \n",
    "    di default max_block_heigth e step sono:\n",
    "        max_block_heigth = address_df[\"use_untill\"].max()\n",
    "        step = 1000\n",
    "    \"\"\"\n",
    "    if mode == \"w\":\n",
    "        if \"level\" not in address_df.columns:\n",
    "            address_df[\"level\"] = 0\n",
    "        address_df.to_csv(nodefile, index=False)\n",
    "        with open(edgefile, \"w\", encoding=\"UTF8\") as tx_file:\n",
    "            tx_file.write(\"from,to,value,effectiveGasPrice,gasUsed,hash,input,blockNumber,level\\n\")\n",
    "        curr_level = 0\n",
    "        curr_level_address_set = set(address_df[\"address\"].values)\n",
    "        curr_level_address_df = address_df\n",
    "        while curr_level < depth:\n",
    "            max_block_heigth = address_df[\"use_untill\"].max()\n",
    "            #step = 1000\n",
    "            step = max_block_heigth//15000\n",
    "            pbar = tqdm(total=step)\n",
    "            new_level = curr_level + 1\n",
    "            #generate new level of edges and nodes\n",
    "            with Manager() as manager:\n",
    "                lock = manager.Lock()\n",
    "                with Pool() as pool:\n",
    "                    items = [(i, lock, store, use_untill, step, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df, pbar) for i in range(0, max_block_heigth+1, step)]\n",
    "                    new_level_address_subdf_list = pool.map(task, items)\n",
    "            new_level_address_df = pd.concat(new_level_address_subdf_list)\n",
    "            #elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\n",
    "            new_level_address_df = new_level_address_df.\\\n",
    "                sort_values([\"use_untill\"]).\\\n",
    "                drop_duplicates(subset=[\"address\"], keep=\"last\").\\\n",
    "                reset_index(drop=True)\\\n",
    "            #elimina da new_level_address_df le righe che hanno address presenti in address_df\n",
    "            new_level_address_df = new_level_address_df[~new_level_address_df[\"address\"].isin(address_df[\"address\"])]\n",
    "            address_df = pd.concat([new_level_address_df, address_df])\n",
    "            new_level_address_df.to_csv(nodefile, mode=\"a\", header=False, index=False)\n",
    "            curr_level += 1\n",
    "            curr_level_address_set = set(new_level_address_df[\"address\"].values)\n",
    "            curr_level_address_df = new_level_address_df\n",
    "            pbar.close()\n",
    "    return curr_level_address_df\n",
    "\n",
    "def task(data):\n",
    "    start, lock, store, use_untill, step, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df, pbar = data\n",
    "    new_level_address_subset = set()\n",
    "    new_level_address_subdf = pd.DataFrame.from_dict({\"address\": [], \"use_untill\": [], \"level\": []})\n",
    "    for block_number in range(min(start+step-1, max_block_heigth), start-1, -1):\n",
    "        block = w3.eth.get_block(block_number)\n",
    "        for transaction in block.transactions[::-1]:\n",
    "            tx = {**w3.eth.get_transaction(transaction.hex()), **w3.eth.get_transaction_receipt(transaction.hex())}\n",
    "            if tx[\"status\"] == 1:\n",
    "                address_to_add = None\n",
    "                if store == \"received\" and use_untill == True:\n",
    "                    if tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                        address_to_add = tx[\"from\"]\n",
    "                elif store == \"received\" and use_untill == False:\n",
    "                    if tx[\"to\"] in curr_level_address_set:\n",
    "                        address_to_add = tx[\"from\"]\n",
    "                elif store == \"sent\" and use_untill == True:\n",
    "                    if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                        address_to_add = tx[\"to\"]\n",
    "                elif store == \"sent\" and use_untill == False:\n",
    "                    if tx[\"from\"] in curr_level_address_set:\n",
    "                        address_to_add = tx[\"to\"]\n",
    "                elif store == \"both\" and use_untill == True:\n",
    "                    if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                        address_to_add = tx[\"to\"]\n",
    "                    elif tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                        address_to_add = tx[\"from\"]\n",
    "                elif store == \"both\" and use_untill == False:\n",
    "                    if tx[\"from\"] in curr_level_address_set:\n",
    "                        address_to_add = tx[\"to\"]\n",
    "                    elif tx[\"to\"] in curr_level_address_set:\n",
    "                        address_to_add = tx[\"from\"]\n",
    "                if address_to_add == None:\n",
    "                    with lock:\n",
    "                        with open(edgefile, \"a\", encoding=\"UTF8\") as tx_file:\n",
    "                            tx_file.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                tx[\"from\"], tx[\"to\"], tx[\"value\"], tx[\"effectiveGasPrice\"], tx[\"gasUsed\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"], new_level\n",
    "                                ))\n",
    "                    if address_to_add not in new_level_address_subset:\n",
    "                        new_level_address_subset.add(address_to_add)\n",
    "                        row = pd.DataFrame.from_dict({\"address\": [address_to_add], \"use_untill\": [block_number], \"level\": [new_level]})\n",
    "                        new_level_address_subdf = pd.concat([new_level_address_subdf, row], ignore_index=True)\n",
    "    pbar.update(1)\n",
    "    return new_level_address_subdf\n",
    "\n",
    "def preprocessing(datafile):\n",
    "    df = pd.read_csv(datafile)\n",
    "    address_df = df.\\\n",
    "        sort_values([\"block_number_remove\"]).\\\n",
    "        drop_duplicates(subset=[\"from\"], keep=\"last\").\\\n",
    "        reset_index(drop=True)\\\n",
    "        [[\"from\", \"block_number_remove\"]].\\\n",
    "        rename({\"from\":\"address\", \"block_number_remove\":\"use_untill\"}, axis=\"columns\")\n",
    "    return address_df\n",
    "\n",
    "\n",
    "eth_url = 'https://mainnet.infura.io/v3/e0a4e987f3ff4f4fa9aa21bb08f09ef5'\n",
    "#bsc_url = \"https://bsc-dataseed.binance.org/\"\n",
    "\n",
    "eth_datafile = 'one_day_exit_scam_eth.csv'\n",
    "#bsc_datafile = 'one_day_exit_scam_bsc.csv'\n",
    "\n",
    "eth_edgefile = 'tx_eth.csv'\n",
    "#bsc_edgefile = 'tx_bsc.csv'\n",
    "\n",
    "eth_nodefile = 'address_eth.csv'\n",
    "#bsc_nodefile = 'address_bsc.csv'\n",
    "\n",
    "eth_w3 = Web3(Web3.HTTPProvider(eth_url))\n",
    "#bsc_w3 = Web3(Web3.HTTPProvider(bsc_url))\n",
    "#bsc_w3.middleware_onion.inject(geth_poa_middleware, layer=0)\n",
    "\n",
    "\n",
    "\n",
    "datafile, edgefile, nodefile, w3 = eth_datafile, eth_edgefile, eth_nodefile, eth_w3\n",
    "#datafile, edgefile, nodefile, w3 = bsc_datafile, bsc_edgefile, bsc_nodefile, bsc_w3\n",
    "\n",
    "multi(preprocessing(datafile), edgefile, nodefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
