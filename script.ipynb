{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install web3 -q\n",
    "%pip install pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from web3.middleware import geth_poa_middleware\n",
    "from multiprocessing import Pool, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(datafile):\n",
    "    df = pd.read_csv(datafile)\n",
    "    block_heigth = df[\"block_number_remove\"].max()\n",
    "    address_df = df.\\\n",
    "        sort_values([\"block_number_remove\"]).\\\n",
    "        drop_duplicates(subset=[\"from\"], keep=\"last\").\\\n",
    "        reset_index(drop=True)\\\n",
    "        [[\"from\", \"block_number_remove\"]].\\\n",
    "        rename({\"from\":\"address\", \"block_number_remove\":\"use_untill\"}, axis=\"columns\")\n",
    "    return address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single(address_df, edgefile, nodefile, w3):\n",
    "    max_block_heigth = address_df[\"use_untill\"].max()\n",
    "    address_set = set(address_df[\"address\"].values)\n",
    "    with open(edgefile, \"w\", encoding=\"UTF8\") as tx_file:\n",
    "        tx_file.write(\"from,to,value,gas,hash,input,blockNumber,transactionIndex\\n\")\n",
    "        for block_number in range(max_block_heigth, -1, -1):\n",
    "            block = w3.eth.get_block(block_number)\n",
    "            for transaction in block.transactions[::-1]:\n",
    "                tx = {**w3.eth.get_transaction(transaction.hex()), **w3.eth.get_transaction_receipt(transaction.hex())}\n",
    "                if tx[\"status\"] == 1 and tx[\"to\"] in address_set and tx[\"to\"] in address_df[address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                    tx_file.write(\"{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                        tx[\"from\"], tx[\"to\"], tx[\"value\"], tx[\"gas\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"], tx[\"transactionIndex\"]\n",
    "                    ))\n",
    "                    if tx[\"from\"] not in address_set:\n",
    "                        address_set.add(tx[\"from\"])\n",
    "                        row = pd.DataFrame.from_dict({\"address\": [tx[\"from\"]], \"use_untill\": [block_number]})\n",
    "                        address_df = pd.concat([address_df, row], ignore_index=True)\n",
    "    address_df.to_csv(nodefile, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi(address_df, edgefile, nodefile, w3, depth=100, mode=\"w\", store=\"received\", use_untill=True): \n",
    "    \"\"\"\n",
    "    depth: (int >=0) ultimo livello da archiviare compreso\n",
    "    mode: (\"w\", \"a\") \n",
    "        \"w\": dal livello 0 a depth compreso sovrascrivendo i file\n",
    "        \"a\": dall'ultimo livello gia' archiviato nei file fino a depth compreso appendendo nei file\n",
    "    store: (\"received\", \"sent\", \"both\")\n",
    "        \"received\": per la creazione di un nuovo livello vengono archiviate solo le transazioni ricevute dagli address del livello corrente\n",
    "        \"sent\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate dagli address del livello corrente\n",
    "        \"both\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate e ricevute dagli address del livello corrente\n",
    "    use_untill: (True, False)\n",
    "        True: le transazioni, una volta filtrate da store, vengono filtrate da block_number <= use_untill\n",
    "        False: le transazioni, una volta filtrate da store, vengono accettate tutte\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_block_heigth e step sono due parametri da poter tarare con dei MA:\n",
    "        siccome nel preprocessing vengono accettati tutti gli address del cvs, max_block_heigth non puo' essere minore di address_df[\"use_untill\"].max(), per evitare di disegnare nel grafo alcuni nodi incorretti\n",
    "        max_block_heigth puo' essere cio' che ci pare al netto del vincolo appena citato solo se \n",
    "    di default max_block_heigth e step sono:\n",
    "        max_block_heigth = address_df[\"use_untill\"].max()\n",
    "        step = 1000\n",
    "    \"\"\"\n",
    "    def task(start, lock):\n",
    "        def subtask():\n",
    "            with lock:\n",
    "                tx_file.write(\"{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                    tx[\"from\"], tx[\"to\"], tx[\"value\"], tx[\"gas\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"], tx[\"transactionIndex\"], new_level\n",
    "                ))\n",
    "            if address_to_add not in new_level_address_subset:\n",
    "                new_level_address_subset.add(address_to_add)\n",
    "                row = pd.DataFrame.from_dict({\"address\": [address_to_add], \"use_untill\": [block_number], \"level\": [new_level]})\n",
    "                new_level_address_subdf = pd.concat([new_level_address_subdf, row], ignore_index=True)\n",
    "            return \n",
    "        new_level_address_subset = set()\n",
    "        new_level_address_subdf = pd.DataFrame.from_dict({\"address\": [], \"use_untill\": [], \"level\": []})\n",
    "        for block_number in range(min(start+step-1, max_block_heigth), start-1, -1):\n",
    "            block = w3.eth.get_block(block_number)\n",
    "            for transaction in block.transactions[::-1]:\n",
    "                tx = {**w3.eth.get_transaction(transaction.hex()), **w3.eth.get_transaction_receipt(transaction.hex())}\n",
    "                if tx[\"status\"] == 1:\n",
    "                    address_to_add = None\n",
    "                    if store == \"received\" and use_untill == True:\n",
    "                        if tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    elif store == \"received\" and use_untill == False:\n",
    "                        if tx[\"to\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    elif store == \"sent\" and use_untill == True:\n",
    "                        if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                    elif store == \"sent\" and use_untill == False:\n",
    "                        if tx[\"from\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                    elif store == \"both\" and use_untill == True:\n",
    "                        if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                        elif tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    elif store == \"both\" and use_untill == False:\n",
    "                        if tx[\"from\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"to\"]\n",
    "                        elif tx[\"to\"] in curr_level_address_set:\n",
    "                            address_to_add = tx[\"from\"]\n",
    "                    if address_to_add != None:\n",
    "                        subtask()\n",
    "        return new_level_address_subdf\n",
    "    if mode == \"w\":\n",
    "        if \"level\" not in address_df.columns:\n",
    "            address_df[\"level\"] = 0\n",
    "        address_df.to_csv(nodefile, index=False)\n",
    "        with open(edgefile, \"w\", encoding=\"UTF8\") as tx_file:\n",
    "            tx_file.write(\"from,to,value,gas,hash,input,blockNumber,transactionIndex,level\\n\")\n",
    "            curr_level = 0\n",
    "            curr_level_address_set = set(address_df[\"address\"].values)\n",
    "            curr_level_address_df = address_df\n",
    "            while curr_level < depth:\n",
    "                max_block_heigth = address_df[\"use_untill\"].max()\n",
    "                step = 1000\n",
    "                # step = max_block_heigth//15000\n",
    "                new_level = curr_level + 1\n",
    "                #generate new level of edges and nodes\n",
    "                with Manager() as manager:\n",
    "                    lock = manager.Lock()\n",
    "                    with Pool() as pool:\n",
    "                        items = [(i, lock) for i in range(0, max_block_heigth+1, step)]\n",
    "                        new_level_address_subdf_list = pool.starmap_async(task, items)\n",
    "                        new_level_address_subdf_list.wait()\n",
    "                new_level_address_df = pd.concat(new_level_address_subdf_list.get())\n",
    "                #elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\n",
    "                new_level_address_df = new_level_address_df.\\\n",
    "                    sort_values([\"use_untill\"]).\\\n",
    "                    drop_duplicates(subset=[\"address\"], keep=\"last\").\\\n",
    "                    reset_index(drop=True)\\\n",
    "                #elimina da new_level_address_df le righe che hanno address presenti in address_df\n",
    "                address_df = pd.concat([new_level_address_df])\n",
    "                new_level_address_df.to_csv(nodefile, mode=\"a\", header=False, index=False)\n",
    "                curr_level += 1\n",
    "                curr_level_address_set = set(new_level_address_df[\"address\"].values)\n",
    "                curr_level_address_df = new_level_address_df\n",
    "    return curr_level_address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_url = 'https://mainnet.infura.io/v3/e0a4e987f3ff4f4fa9aa21bb08f09ef5'\n",
    "bsc_url = \"https://bsc-dataseed.binance.org/\"\n",
    "\n",
    "eth_datafile = 'one_day_exit_scam_eth.csv'\n",
    "bsc_datafile = 'one_day_exit_scam_bsc.csv'\n",
    "\n",
    "eth_edgefile = 'tx_eth.csv'\n",
    "bsc_edgefile = 'tx_bsc.csv'\n",
    "\n",
    "eth_nodefile = 'address_eth.csv'\n",
    "bsc_nodefile = 'address_bsc.csv'\n",
    "\n",
    "eth_w3 = Web3(Web3.HTTPProvider(eth_url))\n",
    "bsc_w3 = Web3(Web3.HTTPProvider(bsc_url))\n",
    "bsc_w3.middleware_onion.inject(geth_poa_middleware, layer=0)\n",
    "\n",
    "eth_address_df = preprocessing(eth_datafile)\n",
    "bsc_address_df = preprocessing(bsc_datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single(eth_address_df, eth_edgefile, eth_nodefile, eth_w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single(bsc_address_df, bsc_edgefile, bsc_nodefile, bsc_w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi(eth_address_df, eth_edgefile, eth_nodefile, eth_w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi(bsc_address_df, eth_edgefile, eth_nodefile, eth_w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ 'gas': 21000, 'gasPrice': 126500000000, 'cumulativeGasUsed': 12490032, 'effectiveGasPrice': 126500000000, 'gasUsed': 21000}\n",
    "\n",
    "{'gas': 21000, 'gasPrice': 22440056776,  'cumulativeGasUsed': 30107260, 'effectiveGasPrice': 22440056776, 'gasUsed': 21000, 'maxFeePerGas': 28093259369, 'maxPriorityFeePerGas': 1500000000, 'accessList': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "3\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s(ide, lock):\n",
    "    with lock:\n",
    "        print(ide)\n",
    "    return (ide, ide)\n",
    "with Manager() as manager:\n",
    "    # create the shared lock\n",
    "    lock = manager.Lock()\n",
    "    # create and configure the process pool\n",
    "    with Pool() as pool:\n",
    "        # prepare task arguments\n",
    "        items = [(i, lock) for i in range(10)]\n",
    "        # issue tasks into the process pool\n",
    "        result = pool.starmap_async(s, items)\n",
    "        # wait for all tasks to finish\n",
    "        result.wait()\n",
    "result.get()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('stable')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "731c43eb56f848c5bca9de05efde814bd49b40cfac306b2f8be57987981a1007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
