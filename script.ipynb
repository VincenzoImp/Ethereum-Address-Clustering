{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install web3 -q\n",
    "%pip install pandas -q\n",
    "%pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from web3.middleware import geth_poa_middleware\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#BSC\\nrpc = ''\\nhttp = 'https://bsc-dataseed.binance.org/' \\ndatafile = 'one_day_exit_scam_bsc.csv'\\nedgefile = 'tx_bsc.csv'\\nnodefile = 'address_bsc.csv'\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3 = None\n",
    "\n",
    "\n",
    "#ETH\n",
    "rpc = '/eth/eth_node/node/geth.ipc'\n",
    "http = 'https://mainnet.infura.io/v3/e0a4e987f3ff4f4fa9aa21bb08f09ef5'\n",
    "datafile = 'data/one_day_exit_scam_eth.csv'\n",
    "edgefile = 'data/tx_eth.csv'\n",
    "nodefile = 'data/address_eth.csv'\n",
    "\n",
    "\"\"\"\n",
    "#BSC\n",
    "rpc = ''\n",
    "http = 'https://bsc-dataseed.binance.org/' \n",
    "datafile = 'one_day_exit_scam_bsc.csv'\n",
    "edgefile = 'tx_bsc.csv'\n",
    "nodefile = 'address_bsc.csv'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(datafile):\n",
    "    df = pd.read_csv(datafile)\n",
    "    address_df = df.\\\n",
    "        sort_values([\"block_number_remove\"]).\\\n",
    "        drop_duplicates(subset=[\"from\"], keep=\"last\").\\\n",
    "        reset_index(drop=True)\\\n",
    "        [[\"from\", \"block_number_remove\"]].\\\n",
    "        rename({\"from\":\"address\", \"block_number_remove\":\"use_untill\"}, axis=\"columns\")\n",
    "    return address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(data):\n",
    "    start, lock, store, use_untill, step, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df = data\n",
    "    new_level_address_subset = set()\n",
    "    new_level_address_subdf = pd.DataFrame.from_dict({\"address\": [], \"use_untill\": [], \"level\": []})\n",
    "    for block_number in range(min(start+step-1, max_block_heigth), start-1, -1):\n",
    "        block = w3.eth.get_block(block_number)\n",
    "        for transaction in block.transactions[::-1]:\n",
    "            tx = w3.eth.get_transaction(transaction.hex())\n",
    "            try:\n",
    "                t_status = tx[\"status\"]\n",
    "            except KeyError:\n",
    "                t_status = None\n",
    "            address_to_add = None\n",
    "            if store == \"received\" and use_untill == True:\n",
    "                if tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                    address_to_add = tx[\"from\"]\n",
    "            elif store == \"received\" and use_untill == False:\n",
    "                if tx[\"to\"] in curr_level_address_set:\n",
    "                    address_to_add = tx[\"from\"]\n",
    "            elif store == \"sent\" and use_untill == True:\n",
    "                if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                    address_to_add = tx[\"to\"]\n",
    "            elif store == \"sent\" and use_untill == False:\n",
    "                if tx[\"from\"] in curr_level_address_set:\n",
    "                    address_to_add = tx[\"to\"]\n",
    "            elif store == \"both\" and use_untill == True:\n",
    "                if tx[\"from\"] in curr_level_address_set and tx[\"from\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                    address_to_add = tx[\"to\"]\n",
    "                elif tx[\"to\"] in curr_level_address_set and tx[\"to\"] in curr_level_address_df[curr_level_address_df[\"use_untill\"]<=block_number][\"address\"].values:\n",
    "                    address_to_add = tx[\"from\"]\n",
    "            elif store == \"both\" and use_untill == False:\n",
    "                if tx[\"from\"] in curr_level_address_set:\n",
    "                    address_to_add = tx[\"to\"]\n",
    "                elif tx[\"to\"] in curr_level_address_set:\n",
    "                    address_to_add = tx[\"from\"]\n",
    "            if address_to_add != None:\n",
    "                tx = {**tx, **w3.eth.get_transaction_receipt(transaction.hex())}\n",
    "                with lock:\n",
    "                    with open(edgefile, \"a\", encoding=\"UTF8\") as tx_file:\n",
    "                        tx_file.write(\"{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                            tx[\"from\"], tx[\"to\"], w3.fromWei(tx[\"value\"], 'ether'), tx[\"effectiveGasPrice\"], tx[\"gasUsed\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"], new_level, t_status\n",
    "                            ))\n",
    "                if address_to_add not in new_level_address_subset:\n",
    "                    new_level_address_subset.add(address_to_add)\n",
    "                    row = pd.DataFrame.from_dict({\"address\": [address_to_add], \"use_untill\": [block_number], \"level\": [new_level]})\n",
    "                    new_level_address_subdf = pd.concat([new_level_address_subdf, row], ignore_index=True)\n",
    "    return new_level_address_subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi(address_df, chain, depth=2, mode=\"w\", store=\"received\", use_untill=True): \n",
    "    \"\"\"\n",
    "    depth: (int >=0) ultimo livello da archiviare compreso\n",
    "    mode: (\"w\", \"a\") \n",
    "        \"w\": dal livello 0 a depth compreso sovrascrivendo i file\n",
    "        \"a\": dall'ultimo livello gia' archiviato nei file fino a depth compreso appendendo nei file\n",
    "    store: (\"received\", \"sent\", \"both\")\n",
    "        \"received\": per la creazione di un nuovo livello vengono archiviate solo le transazioni ricevute dagli address del livello corrente\n",
    "        \"sent\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate dagli address del livello corrente\n",
    "        \"both\": per la creazione di un nuovo livello vengono archiviate solo le transazioni inviate e ricevute dagli address del livello corrente\n",
    "    use_untill: (True, False)\n",
    "        True: le transazioni, una volta filtrate da store, vengono filtrate da block_number <= use_untill\n",
    "        False: le transazioni, una volta filtrate da store, vengono accettate tutte\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_block_heigth e step sono due parametri da poter tarare con dei MA:\n",
    "        siccome nel preprocessing vengono accettati tutti gli address del cvs, max_block_heigth non puo' essere minore di address_df[\"use_untill\"].max(), per evitare di disegnare nel grafo alcuni nodi incorretti\n",
    "        max_block_heigth puo' essere cio' che ci pare al netto del vincolo appena citato solo se \n",
    "    di default max_block_heigth e step sono:\n",
    "        max_block_heigth = address_df[\"use_untill\"].max()\n",
    "        step = 1000\n",
    "    \"\"\"\n",
    "    global w3\n",
    "    if chain == \"eth\":\n",
    "        w3 = Web3(Web3.IPCProvider(rpc))\n",
    "    elif chain == \"bsc\":\n",
    "        w3 = Web3(Web3.HTTPProvider(rpc))\n",
    "        w3.middleware_onion.inject(geth_poa_middleware, layer=0)\n",
    "    if mode == \"w\":\n",
    "        if \"level\" not in address_df.columns:\n",
    "            address_df[\"level\"] = 0\n",
    "        address_df.to_csv(nodefile, index=False)\n",
    "        with open(edgefile, \"w\", encoding=\"UTF8\") as tx_file:\n",
    "            tx_file.write(\"from,to,value,effectiveGasPrice,gasUsed,hash,input,blockNumber,level,status\\n\")\n",
    "        curr_level = 0\n",
    "        curr_level_address_set = set(address_df[\"address\"].values)\n",
    "        curr_level_address_df = address_df\n",
    "        while curr_level < depth:\n",
    "            max_block_heigth = address_df[\"use_untill\"].max()\n",
    "            step = 10000\n",
    "            new_level = curr_level + 1\n",
    "            #generate new level of edges and nodes\n",
    "            with Manager() as manager:\n",
    "                lock = manager.Lock()\n",
    "                with Pool(25) as pool:\n",
    "                    items = [(i, lock, store, use_untill, step, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df) for i in range(0, max_block_heigth+1, step)]\n",
    "                    #new_levemax_block_heigth_address_subdf_list = list(tqdm(pool.imap(task, items), total=len(items)))\n",
    "                    for new_level_address_subdf in tqdm(pool.imap(task, items), total=len(items)): \n",
    "                        new_level_address_df = pd.concat([new_level_address_subdf])\n",
    "            #elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\n",
    "            new_level_address_df = new_level_address_df.\\\n",
    "                sort_values([\"use_untill\"]).\\\n",
    "                drop_duplicates(subset=[\"address\"], keep=\"last\").\\\n",
    "                reset_index(drop=True)\\\n",
    "            #elimina da new_level_address_df le righe che hanno address presenti in address_df\n",
    "            new_level_address_df = new_level_address_df[~new_level_address_df[\"address\"].isin(address_df[\"address\"])]\n",
    "            address_df = pd.concat([new_level_address_df, address_df])\n",
    "            new_level_address_df.to_csv(nodefile, mode=\"a\", header=False, index=False)\n",
    "            curr_level += 1\n",
    "            curr_level_address_set = set(new_level_address_df[\"address\"].values)\n",
    "            curr_level_address_df = new_level_address_df\n",
    "    return curr_level_address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi(preprocessing(datafile), \"eth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
