{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install web3 -q\n",
    "#%pip install pandas -q\n",
    "#%pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from web3.middleware import geth_poa_middleware\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = 'eth'\n",
    "query_mode = 'http'\n",
    "core_number = 25\n",
    "chunk_size = 10000\n",
    "w3 = None\n",
    "data = {\n",
    "    'eth': {\n",
    "        'rpc': '/eth/eth_node/node/geth.ipc',\n",
    "        'http': 'https://mainnet.infura.io/v3/e0a4e987f3ff4f4fa9aa21bb08f09ef5',\n",
    "        'datafile': 'data/one_day_exit_scam_eth.csv',\n",
    "        'edgefile': 'data/transactions_eth.csv',\n",
    "        'nodefile': 'data/addresses_eth.csv',\n",
    "        'logfile': 'data/chunk_log_eth.csv'\n",
    "    },\n",
    "    'bsc': {\n",
    "        'rpc': '',\n",
    "        'http': 'https://bsc-dataseed.binance.org/',\n",
    "        'datafile': 'data/one_day_exit_scam_bsc.csv',\n",
    "        'edgefile': 'data/transactions_bsc.csv',\n",
    "        'nodefile': 'data/addresses_bsc.csv',\n",
    "        'logfile': 'data/chunk_log_bsc.csv'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w3():\n",
    "    tmp = data[chain][query_mode]\n",
    "    w3_data = {\n",
    "        'eth': {\n",
    "            'rpc': Web3(Web3.IPCProvider(tmp)),\n",
    "            'http': Web3(Web3.HTTPProvider(tmp))\n",
    "        },\n",
    "        'bsc': {\n",
    "            'rpc': None,\n",
    "            'http': Web3(Web3.HTTPProvider(tmp)).middleware_onion.inject(geth_poa_middleware, layer=0)\n",
    "        } \n",
    "    }\n",
    "    return w3_data[chain][query_mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_edges(rows):\n",
    "    with open(data[chain]['edgefile'], \"a\", encoding=\"UTF8\") as tx_file:\n",
    "        csv.writer(tx_file).writerows(rows)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(row):\n",
    "    with open(data[chain]['logfile'], \"a\", encoding=\"UTF8\") as log_file:\n",
    "        csv.writer(log_file).writerow(row)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    datafile = data[chain]['datafile']\n",
    "    df = pd.read_csv(datafile, dtype={'address':str, 'block_number_remove':int})\n",
    "    address_df = df.\\\n",
    "        sort_values([\"block_number_remove\"]).\\\n",
    "        drop_duplicates(subset=[\"from\"], keep=\"last\").\\\n",
    "        reset_index(drop=True)\\\n",
    "        [[\"from\", \"block_number_remove\"]].\\\n",
    "        rename({\"from\":\"address\", \"block_number_remove\":\"use_untill\"}, axis=\"columns\")\n",
    "    address_df[\"level\"] = 0\n",
    "    address_df = address_df.astype({'address': str, 'use_untill':int, 'level':int})\n",
    "    return address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(parameters):\n",
    "    chunkID, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df = parameters\n",
    "    rows_to_write = []\n",
    "    new_level_address_subset = set()\n",
    "    new_level_address_subdf = pd.DataFrame.from_dict({\"address\": [], \"use_untill\": [], \"level\": []}).astype({'address': str, 'use_untill':int, 'level':int})\n",
    "    for block_number in range(chunkID, min(chunkID+chunk_size, max_block_heigth+1)):\n",
    "        block = w3.eth.get_block(block_number)\n",
    "        local_filtered_curr_level_addresses = curr_level_address_df[curr_level_address_df[\"use_untill\"]>=block_number][\"address\"].values\n",
    "        for transaction in block.transactions:\n",
    "            tx = w3.eth.get_transaction(transaction.hex())\n",
    "            if tx[\"to\"] in curr_level_address_set and tx[\"to\"] in local_filtered_curr_level_addresses:\n",
    "                address_to_add = tx[\"from\"]\n",
    "                tx = {**tx, **w3.eth.get_transaction_receipt(transaction.hex())}\n",
    "                rows_to_write.append(\"{},{},{},{},{},{},{},{},{},{}\".format(\n",
    "                    tx[\"from\"], tx[\"to\"], w3.fromWei(tx[\"value\"], 'ether'), tx[\"effectiveGasPrice\"], tx[\"gasUsed\"], tx[\"hash\"].hex(), tx[\"input\"][:10], tx[\"blockNumber\"], new_level, tx[\"status\"]\n",
    "                ).split(','))\n",
    "                if address_to_add not in new_level_address_subset:\n",
    "                    new_level_address_subset.add(address_to_add)\n",
    "                    row = pd.DataFrame.from_dict({\"address\": [address_to_add], \"use_untill\": [block_number], \"level\": [new_level]}).astype({'address': str, 'use_untill':int, 'level':int})\n",
    "                    new_level_address_subdf = pd.concat([new_level_address_subdf, row], ignore_index=True)\n",
    "    write_edges(rows_to_write)\n",
    "    return chunkID, new_level_address_subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi(depth, store_mode='w', log=True):\n",
    "    \"\"\"\n",
    "    depth: (int >=0) ultimo livello da archiviare compreso\n",
    "    max_block_heigth e chunk_size sono due parametri da poter tarare con dei MA:\n",
    "        siccome nel preprocessing vengono accettati tutti gli address del cvs, max_block_heigth non puo' essere minore di address_df[\"use_untill\"].max(), per evitare di disegnare nel grafo alcuni nodi incorretti\n",
    "        max_block_heigth puo' essere cio' che ci pare al netto del vincolo appena citato solo se \n",
    "    di default max_block_heigth e chunk_size sono:\n",
    "        max_block_heigth = address_df[\"use_untill\"].max()\n",
    "        chunk_size = 1000\n",
    "    \"\"\"\n",
    "    global w3\n",
    "    w3 = get_w3()\n",
    "    if store_mode == \"w\":\n",
    "        if log == True:\n",
    "            with open(data[chain]['logfile'], \"w\", encoding=\"UTF8\") as log_file:\n",
    "                csv.writer(log_file).writerow(\"level,chunkID,chunk_size\".split(','))\n",
    "        address_df = preprocessing()\n",
    "        address_df.to_csv(data[chain]['nodefile'], index=False)\n",
    "        with open(data[chain]['edgefile'], \"w\", encoding=\"UTF8\") as tx_file:\n",
    "            csv.writer(tx_file).writerow(\"from,to,value,effectiveGasPrice,gasUsed,hash,input,blockNumber,level,status\".split(','))\n",
    "    if store_mode == \"a\":\n",
    "        address_df = pd.read_csv(data[chain]['nodefile'])\n",
    "\n",
    "    curr_level = int(address_df[\"level\"].max())\n",
    "    curr_level_address_set = set(address_df[\"address\"].values)\n",
    "    curr_level_address_df = address_df.copy()\n",
    "    while curr_level < depth:\n",
    "        max_block_heigth = int(curr_level_address_df[\"use_untill\"].max())\n",
    "        new_level = curr_level + 1\n",
    "        #generate new level of edges and nodes\n",
    "        with Pool(core_number) as pool:\n",
    "            items = [(chunkID, max_block_heigth, new_level, curr_level_address_set, curr_level_address_df) for chunkID in range(0, max_block_heigth+1, chunk_size)]\n",
    "            new_level_address_df = pd.DataFrame.from_dict({\"address\":[], \"use_untill\":[], \"level\":[]}).astype({'address': str, 'use_untill':int, 'level':int})\n",
    "            for chunkID, new_level_address_subdf in tqdm(pool.imap(task, items), total=len(items)):\n",
    "                new_level_address_df = pd.concat([new_level_address_df, new_level_address_subdf])\n",
    "                if log == True:\n",
    "                    write_log('{},{},{}'.format(new_level, chunkID, chunk_size).split(','))\n",
    "        #elimina da new_level_address_df tutte le righe con address ripeturi e con use_untill che non è massimo tra i doppioni\n",
    "        new_level_address_df = new_level_address_df.\\\n",
    "            sort_values([\"use_untill\"]).\\\n",
    "            drop_duplicates(subset=[\"address\"], keep=\"last\").\\\n",
    "            reset_index(drop=True)\n",
    "        #elimina da new_level_address_df le righe che hanno address presenti in address_df\n",
    "        new_level_address_df = new_level_address_df[~new_level_address_df[\"address\"].isin(address_df[\"address\"])]\n",
    "        address_df = pd.concat([address_df, new_level_address_df])\n",
    "        new_level_address_df.to_csv(data[chain]['nodefile'], mode=\"a\", header=False, index=False)\n",
    "        curr_level += 1\n",
    "        curr_level_address_set = set(new_level_address_df[\"address\"].values)\n",
    "        curr_level_address_df = new_level_address_df.copy()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "multi(depth=1, store_mode='w', log=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('stable')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "731c43eb56f848c5bca9de05efde814bd49b40cfac306b2f8be57987981a1007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
